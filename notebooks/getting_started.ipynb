{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f4e294",
   "metadata": {},
   "source": [
    "# DSPy Getting Started Notebook\n",
    "\n",
    "This notebook demonstrates the basics of using DSPy for language model programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdc1ee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy version: 2.6.27\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import requests\n",
    "\n",
    "print(\"DSPy version:\", dspy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "663d2454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running!\n",
      "Available models: 7\n",
      "   - gemma3:1b\n",
      "   - mistral-small3.2:24b\n",
      "   - llava:7b\n",
      "   - deepseek-r1:70b\n",
      "   - deepseek-r1:32b\n",
      "   - gemma3:27b\n",
      "   ... and 1 more\n"
     ]
    }
   ],
   "source": [
    "def check_ollama():\n",
    "    try:\n",
    "        # Check if Ollama server is running\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(\"Ollama server is running!\")\n",
    "            print(f\"Available models: {len(models)}\")\n",
    "            for model in models[:6]:  # Show first 6 models\n",
    "                print(f\"   - {model['name']}\")\n",
    "            if len(models) > 6:\n",
    "                print(f\"   ... and {len(models) - 6} more\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Ollama server not responding\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"Ollama server is not running\")\n",
    "        print(\"Start it with: ollama serve\")\n",
    "        print(\"Install models with: ollama pull gemma3:1b\")\n",
    "        return False\n",
    "\n",
    "ollama_available = check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096be02",
   "metadata": {},
   "source": [
    "## 1. Setting up a Language Model\n",
    "\n",
    "First configure DSPy to use a language model. This notebook is configured to use **Ollama** as the default backend, which allows you to run models locally.\n",
    "\n",
    "### Prerequisites for Ollama:\n",
    "1. Install Ollama: `curl -fsSL https://ollama.ai/install.sh | sh`\n",
    "2. Start Ollama server: `ollama serve`\n",
    "3. Install a model: `ollama pull gemma3:1b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd41a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to Ollama...\n"
     ]
    }
   ],
   "source": [
    "# Configure language model - Ollama as default\n",
    "try:\n",
    "    print(\"Attempting to connect to Ollama...\")\n",
    "    lm = dspy.LM(\n",
    "        \"ollama_chat/gemma3:1b\",\n",
    "        api_base=\"http://localhost:11434\",\n",
    "        api_key=\"\",\n",
    "        model_type=\"chat\"\n",
    "    )\n",
    "    dspy.configure(lm=lm)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Ollama: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10c4ba",
   "metadata": {},
   "source": [
    "## 2. Creating a Basic Signature\n",
    "\n",
    "DSPy signatures define the input-output behavior of language model modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc34dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA predictor created!\n"
     ]
    }
   ],
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Create a predictor\n",
    "qa_predictor = dspy.Predict(BasicQA)\n",
    "\n",
    "print(\"QA predictor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53ec60",
   "metadata": {},
   "source": [
    "## 3. Testing the Basic QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8e114d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the capital of France?\n",
      "A: Paris\n",
      "--------------------------------------------------\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "A: William Shakespeare\n",
      "--------------------------------------------------\n",
      "Q: What is 2 + 2?\n",
      "A: 4\n",
      "--------------------------------------------------\n",
      "Q: What is the largest planet in our solar system?\n",
      "A: Jupiter\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with some questions\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"What is the largest planet in our solar system?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    try:\n",
    "        result = qa_predictor(question=question)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {result.answer}\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with question '{question}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34202a21",
   "metadata": {},
   "source": [
    "## 4. Creating a More Complex Signature\n",
    "\n",
    "Create a signature for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07457dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment classifier created!\n"
     ]
    }
   ],
   "source": [
    "class SentimentClassification(dspy.Signature):\n",
    "    \"\"\"Classify the sentiment of a given text as positive, negative, or neutral.\"\"\"\n",
    "    text = dspy.InputField(desc=\"text to classify\")\n",
    "    sentiment = dspy.OutputField(desc=\"positive, negative, or neutral\")\n",
    "\n",
    "# Create sentiment classifier\n",
    "sentiment_classifier = dspy.Predict(SentimentClassification)\n",
    "\n",
    "print(\"Sentiment classifier created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc52281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/14 14:08:45 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this movie! It's absolutely fantastic.\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Text: This is terrible. I hate it.\n",
      "Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Text: The weather is okay today.\n",
      "Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Text: This product exceeded my expectations!\n",
      "Sentiment: positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test sentiment classification\n",
    "texts = [\n",
    "    \"I love this movie! It's absolutely fantastic.\",\n",
    "    \"This is terrible. I hate it.\",\n",
    "    \"The weather is okay today.\",\n",
    "    \"This product exceeded my expectations!\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    try:\n",
    "        result = sentiment_classifier(text=text)\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment: {result.sentiment}\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with text '{text}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a0046",
   "metadata": {},
   "source": [
    "## 5. Chain of Thought Reasoning\n",
    "\n",
    "DSPy supports chain of thought reasoning with `dspy.ChainOfThought`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a981dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math solver with chain of thought created!\n"
     ]
    }
   ],
   "source": [
    "class MathProblem(dspy.Signature):\n",
    "    \"\"\"Solve a math word problem step by step.\"\"\"\n",
    "    problem = dspy.InputField(desc=\"math word problem\")\n",
    "    solution = dspy.OutputField(desc=\"step-by-step solution with final answer\")\n",
    "\n",
    "# Create a chain of thought predictor\n",
    "math_solver = dspy.ChainOfThought(MathProblem)\n",
    "\n",
    "print(\"Math solver with chain of thought created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e56c1df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: Sarah has 15 apples. She gives 3 apples to her friend and buys 8 more apples. How many apples does Sarah have now?\n",
      "\n",
      "Reasoning: Sarah starts with 15 apples. She gives away 3, so she has 15 - 3 = 12 apples. Then she buys 8 more, so she has 12 + 8 = 20 apples.\n",
      "\n",
      "Solution: Sarah now has 20 apples.\n"
     ]
    }
   ],
   "source": [
    "# Test math problem solving\n",
    "problem = \"Sarah has 15 apples. She gives 3 apples to her friend and buys 8 more apples. How many apples does Sarah have now?\"\n",
    "\n",
    "try:\n",
    "    result = math_solver(problem=problem)\n",
    "    print(f\"Problem: {problem}\")\n",
    "    print(f\"\\nReasoning: {result.reasoning}\")\n",
    "    print(f\"\\nSolution: {result.solution}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error solving math problem: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281086c",
   "metadata": {},
   "source": [
    "## 6. Optimization Example\n",
    "\n",
    "DSPy includes powerful optimization features that can automatically improve your prompts and model performance. Let's explore a basic example using `BootstrapFewShot` optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1075ee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 training examples for optimization\n",
      "\n",
      "Sample training example:\n",
      "Question: What is the tallest mountain in the world?\n",
      "Expected Answer: Mount Everest\n"
     ]
    }
   ],
   "source": [
    "# First, let's create some training data for optimization\n",
    "training_data = [\n",
    "    {\"question\": \"What is the tallest mountain in the world?\", \"answer\": \"Mount Everest\"},\n",
    "    {\"question\": \"Who developed the theory of relativity?\", \"answer\": \"Albert Einstein\"},\n",
    "    {\"question\": \"What is the main language spoken in Brazil?\", \"answer\": \"Portuguese\"},\n",
    "    {\"question\": \"Which planet is known as the Red Planet?\", \"answer\": \"Mars\"},\n",
    "    {\"question\": \"What is the hardest natural substance?\", \"answer\": \"Diamond\"},\n",
    "    {\"question\": \"Who is the author of '1984'?\", \"answer\": \"George Orwell\"},\n",
    "    {\"question\": \"What is the boiling point of water in Celsius?\", \"answer\": \"100\"},\n",
    "    {\"question\": \"What is the currency of Japan?\", \"answer\": \"Yen\"}\n",
    "]\n",
    "\n",
    "# Convert to DSPy format\n",
    "train_examples = [dspy.Example(question=item[\"question\"], answer=item[\"answer\"]).with_inputs(\"question\") \n",
    "                  for item in training_data]\n",
    "\n",
    "print(f\"Created {len(train_examples)} training examples for optimization\")\n",
    "print(\"\\nSample training example:\")\n",
    "print(f\"Question: {train_examples[0].question}\")\n",
    "print(f\"Expected Answer: {train_examples[0].answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab1c836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer configured!\n",
      "Metric: Answer correctness based on keyword matching\n",
      "Strategy: BootstrapFewShot with up to 4 examples\n"
     ]
    }
   ],
   "source": [
    "# Set up optimization using BootstrapFewShot\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Define a simple metric to evaluate answers\n",
    "def answer_correctness_metric(example, pred, trace=None):\n",
    "    \"\"\"Simple metric that checks if the predicted answer contains key words from the expected answer.\"\"\"\n",
    "    predicted = pred.answer.lower().strip()\n",
    "    expected = example.answer.lower().strip()\n",
    "    \n",
    "    # Simple check: if the expected answer is contained in the prediction or vice versa\n",
    "    return expected in predicted or predicted in expected\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = BootstrapFewShot(\n",
    "    metric=answer_correctness_metric,\n",
    "    max_bootstrapped_demos=4,  # Number of examples to use for few-shot\n",
    "    max_labeled_demos=4        # Maximum labeled examples to consider\n",
    ")\n",
    "\n",
    "print(\"Optimizer configured!\")\n",
    "print(\"Metric: Answer correctness based on keyword matching\")\n",
    "print(\"Strategy: BootstrapFewShot with up to 4 examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f6dd3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing the QA predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:00<00:00, 1978.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
      "Optimization completed!\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Original vs Optimized QA System\n",
      "============================================================\n",
      "\n",
      "Question: What is the capital of Italy?\n",
      "Original: Rome\n",
      "Optimized: Rome\n",
      "----------------------------------------\n",
      "\n",
      "Question: Who discovered penicillin?\n",
      "Original: Alexander Fleming discovered penicillin.\n",
      "Optimized: Alexander Fleming\n",
      "----------------------------------------\n",
      "\n",
      "Question: What is the largest mammal?\n",
      "Original: The blue whale\n",
      "Optimized: Blue Whale\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile/optimize the QA predictor\n",
    "print(\"Optimizing the QA predictor...\")\n",
    "try:\n",
    "    optimized_qa = optimizer.compile(qa_predictor, trainset=train_examples[:6])  # Use 6 examples for training\n",
    "    print(\"Optimization completed!\")\n",
    "    \n",
    "    # Test both original and optimized versions\n",
    "    test_questions = [\n",
    "        \"What is the capital of Italy?\",\n",
    "        \"Who discovered penicillin?\",\n",
    "        \"What is the largest mammal?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON: Original vs Optimized QA System\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        \n",
    "        # Original predictor\n",
    "        try:\n",
    "            original_result = qa_predictor(question=question)\n",
    "            print(f\"Original: {original_result.answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Original: Error - {e}\")\n",
    "        \n",
    "        # Optimized predictor\n",
    "        try:\n",
    "            optimized_result = optimized_qa(question=question)\n",
    "            print(f\"Optimized: {optimized_result.answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Optimized: Error - {e}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed: {e}\")\n",
    "    print(\"This might happen if the model doesn't support the optimization strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320c7a8",
   "metadata": {},
   "source": [
    "### Other DSPy Optimizers\n",
    "\n",
    "DSPy offers several other optimization strategies:\n",
    "- **`LabeledFewShot`**: Uses provided examples directly\n",
    "- **`COPRO`**: Coordinate ascent prompt optimization  \n",
    "- **`MIPRO`**: Multi-prompt instruction optimization\n",
    "- **`BayesianSignatureOptimizer`**: Bayesian optimization of signatures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-experiments-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
